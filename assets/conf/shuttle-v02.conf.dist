input {

   file {
	path => "${LOG_BASE_DIR}/${AUDIT_FILES}*"
    codec => multiline {
      # any line  starting with ";" should be merged with the previous line
      pattern => "^\";\""
      what => "previous"
    }
	start_position => "beginning"
	type => "audit"
		
	# in  SINCE_DB is 'sincedb_path => "/dev/null"', then logstash restart parse on all files
	${SINCE_DB}
   }
  
   file {
	  path => "${LOG_BASE_DIR}/${USERS_FILES}*"
	  start_position => "beginning"
	  type => "users"
	  
	  # in  SINCE_DB is 'sincedb_path => "/dev/null"', then logstash restart parse on all files
	  ${SINCE_DB}
	  
   }

}

filter {
    if [message] =~ /^\s*$/ { drop { } } #drop events null rows

    if [type] == "audit"{
	mutate {
		gsub => ["message", "^'", ""]   # remove ' if exist on first char
	}
	fingerprint {
		source => "message"
		target => "fingerprint"
		method => "SHA1"
		key => "shuttle $ audit "  # keypassphrase to generate the fingerprint SHA1, dot not changeit
		base64encode => true
	}
        csv {   # define audit csv file, headers, with additionnal columns from logstash :"DURATION","fingerprint","CLIENT","FORMATED_START_DATE","FORMATED_END_DATE"
		separator => ";"
		columns => ["THREAD","SESSION","IP","USER","LANG","ORIGIN","START_DATE","END_DATE","STATUS","ERROR","CALL","REPO","PROJECT","TARGET","PARAMS","DURATION_S","DURATION","fingerprint","CLIENT","FORMATED_START_DATE","FORMATED_END_DATE"]
		remove_field => ["message"] # remove the full message source
	}

        if ( "Invalid credentials" in [ERROR] ){ # grafana does not allow to filter tag '' value , so replace '' by 'null'
	    mutate {
		replace => ["SESSION",'null']
	        replace => ["USER",'null']
		replace => ["IP",'null']
	        replace => ["ORIGIN",'null']
		replace => ["REPO",'null']
	        replace => ["PROJECT",'null']
	        replace => ["TARGET",'null']
	        replace => ["PARAMS",'null']
		}
	}
        if ( "CEST" in [START_DATE]){ # Some logs can be in Center Europe Summer Time, so write it in CET +0200

		mutate { # 'Jan 6, 2018 12:50:16 AM CEST' become 'Jan 6, 2018 12:50:16 AM +0200'
			gsub => ["START_DATE", "CEST", "+0200"]
			gsub => ["END_DATE", "CEST", "+0200"]
		}
	       
	    date { #Jan 6, 2018 12:50:16 AM +0200 is converted from string type into universal date for elasticsearch 
			match => ["START_DATE","MMM dd, YYYY hh:mm:ss aa Z"]
			target => "START_DATE"
	    }
            date {
			match => ["END_DATE","MMM dd, YYYY hh:mm:ss aa Z"]
			target => "END_DATE"
	    }
		
        } else if (  "CET" in [START_DATE]){
		date {    #Jan 6, 2018 12:50:16 AM CET	
			match => ["START_DATE","MMM dd, YYYY hh:mm:ss aa ZZZ"]
			target => "START_DATE"
		}
		
		date {
			match => ["END_DATE","MMM dd, YYYY hh:mm:ss aa ZZZ"]
			target => "END_DATE"
		}
		 
       } else { # ISO8601 : 2018-11-20 11:10:06+01:00
            date { 
                     match => [ "START_DATE", "YYYY-MM-dd HH:mm:ssZZ" ]
                     target => "START_DATE"
            }
            date { 
                     match => [ "END_DATE", "YYYY-MM-dd HH:mm:ssZZ" ]
                     target => "END_DATE"
            }
       }
	
	mutate { replace => ["CLIENT","${STACK_CLIENT}"] }
	ruby { code => "event.set('DURATION_S', event.get('END_DATE') - event.get('START_DATE'))" }  # date diff to compute duration in secondes 
	ruby { code => "event.set('DURATION', Time.at(event.get('DURATION_S')).utc.strftime('%H:%M:%S'))" }
	 
     }
     if [type] == "users"{ 
		
        # warning : if TZ is not CEST, the filename must contain the TZ info. But not implemented yet
        grok { match => ["path", "(?<ADDED_DATE>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}_%{HOUR}-%{MINUTE}-%{SECOND})"]	} # get info form filename
        mutate { replace => ["ADDED_DATE","%{ADDED_DATE} +0200"] } # Add CEST timezone for declared users 
	   
        csv { # define user csv file, headers, with additionnal columns from logstash: "ADDED_DATE","COUNT","CLIENT"
			separator => ","
			columns => ["USER","ADDED_DATE","COUNT","CLIENT"]
			remove_field => ["message"]
        }

        mutate { replace => ["CLIENT","${STACK_CLIENT}"] }
        mutate { replace => ["COUNT", 0] }
	#mutate { replace => ["type",'audit'] } 
		
	date{         # Convert ADDED_DATE from string  into date type
		match => ["ADDED_DATE","YYYY-MM-dd_HH-mm-ss Z"]
		target => "ADDED_DATE"
	}
     }
     # remove some internal tags, not usable for us , unless we have multiple host
     mutate { remove_field => [ "host", "@version" ] }
}
output{
	if [fingerprint]{ # exist only for audit
    	elasticsearch{
		    ssl => ${EL_SSL}
			ssl_certificate_verification => ${EL_SSL_VERIF_CERT}
			hosts => "${EL_HOST}:${EL_PORT}"
			index => "${EL_INDEX}"
			document_id=>"%{fingerprint}"
			user => ${EL_USER} # logstash user for elasticsearch
			password=>${EL_PASSWORD} #logstash password for elasticsearch
	    }
	} else { #for user, force id 
       	elasticsearch{
			ssl => ${EL_SSL}
			ssl_certificate_verification => ${EL_SSL_VERIF_CERT}
			hosts => "${EL_HOST}:${EL_PORT}"
			index => "${EL_INDEX}"
			document_id=>"%{USER}.%{ADDED_DATE}.%{CLIENT}"
			document_type=>"audit" # force to store tags users into the same index  _type  like audit to allow dashboard
			user => ${EL_USER} # logstash user for elasticsearch
			password=>${EL_PASSWORD} #logstash password for elasticsearch
	    }	
	}

}

