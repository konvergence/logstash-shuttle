input {

   file {
    path => "${LOG_BASE_DIR}/${AUDIT_FILES}*"
    codec => multiline {
      # any line not starting ^\"[a-zA-Z0-9\_\-]+-[0-9]+\";  is a previous line
      pattern => "^\"[a-zA-Z0-9\_\-]+-[0-9]+\";"
      negate => true
      what => "previous"
    }
    start_position => "beginning"
    type => "audit"

    # in  SINCE_DB is 'sincedb_path => "/dev/null"', then logstash restart parse on all files
    ${SINCE_DB}
   }

   file {
      path => "${LOG_BASE_DIR}/${USERS_FILES}*"
      start_position => "beginning"
      type => "users"

      # in  SINCE_DB is 'sincedb_path => "/dev/null"', then logstash restart parse on all files
      ${SINCE_DB}

   }

   file { # see https://dzone.com/articles/using-multiple-grok-statements
      path => "${LOG_BASE_DIR}/${SHUTTLE_FILES}*"

      codec => multiline {
        # any line not starting "^[A-Z\b]{5} \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3} ";  is a previous line
        pattern => "^[ ]*[A-Z]+ \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3} "
        negate => true
        what => "previous"
      }
      start_position => "beginning"
      type => "shuttle"

      # in  SINCE_DB is 'sincedb_path => "/dev/null"', then logstash restart parse on all files
      ${SINCE_DB}

   }

}

filter {
    if [message] =~ /^\s*$/ { drop { } } #drop events null rows

    if [type] == "audit"{
        mutate {
            gsub => ["message", "^'", ""]   # remove ' if exist on first char
        }

        fingerprint {
            source => "message"
            target => "fingerprint"
            method => "SHA1"
            key => "shuttle $ audit "  # keypassphrase to generate the fingerprint SHA1, dot not changeit
            base64encode => true
        }



        csv {   # define audit csv file, headers, with additionnal columns from logstash :"DURATION","fingerprint","CLIENT","FORMATED_START_DATE","FORMATED_END_DATE"
            separator => ";"
            columns => ["THREAD","SESSION","IP","USER","LANG","ORIGIN","START_DATE","END_DATE","STATUS","ERROR","CALL","REPO","PROJECT","TARGET","PARAMS","DURATION_S","DURATION","fingerprint","CLIENT","FORMATED_START_DATE","FORMATED_END_DATE"]
            remove_field => ["message"] # remove the full message source
        }

        # grafana does not allow to filter tag '' value , so replace '' by 'null'
        if ! [SESSION] {
            mutate {
                 replace => ["SESSION",'null']
            }
        }

        if ! [USER] {
            mutate {
                 replace => ["USER",'null']
            }
        }

        if ! [IP] {
            mutate {
                 replace => ["IP",'null']
            }
        }

        if ! [ORIGIN] {
            mutate {
                 replace => ["ORIGIN",'null']
            }
        }

        if ! [REPO] {
            mutate {
                 replace => ["REPO",'null']
            }
        }

        if ! [PROJECT] {
            mutate {
                 replace => ["PROJECT",'null']
            }
        }
        if ! [TARGET] {
            mutate {
                 replace => ["TARGET",'null']
            }
        }
        if ! [PARAMS] {
            mutate {
                 replace => ["PARAMS",'null']
            }
        }



        if ( "CEST" in [START_DATE]){ # Some logs can be in Center Europe Summer Time, so write it in CET +0200

        mutate { # 'Jan 6, 2018 12:50:16 AM CEST' become 'Jan 6, 2018 12:50:16 AM +0200'
            gsub => ["START_DATE", "CEST", "+0200"]
            gsub => ["END_DATE", "CEST", "+0200"]
        }

        date { #Jan 6, 2018 12:50:16 AM +0200 is converted from string type into universal date for elasticsearch
            match => ["START_DATE","MMM dd, YYYY hh:mm:ss aa Z"]
            target => "START_DATE"
        }
            date {
            match => ["END_DATE","MMM dd, YYYY hh:mm:ss aa Z"]
            target => "END_DATE"
        }

        } else if (  "CET" in [START_DATE]){
        date {    #Jan 6, 2018 12:50:16 AM CET
            match => ["START_DATE","MMM dd, YYYY hh:mm:ss aa ZZZ"]
            target => "START_DATE"
        }

        date {
            match => ["END_DATE","MMM dd, YYYY hh:mm:ss aa ZZZ"]
            target => "END_DATE"
        }

        } else { # ISO8601 : 2018-11-20 11:10:06+01:00
            date {
                     match => [ "START_DATE", "YYYY-MM-dd HH:mm:ssZZ" ]
                     target => "START_DATE"
            }
            date {
                     match => [ "END_DATE", "YYYY-MM-dd HH:mm:ssZZ" ]
                     target => "END_DATE"
            }
        }

        mutate { replace => ["CLIENT","${STACK_CLIENT}"] }
        ruby { code => "event.set('DURATION_S', event.get('END_DATE') - event.get('START_DATE'))" }  # date diff to compute duration in secondes
        ruby { code => "event.set('DURATION', Time.at(event.get('DURATION_S')).utc.strftime('%H:%M:%S'))" }

        # add geoip informations
        geoip {
            source => "IP"
            database => "/config-dir/GeoLite2-City.mmdb"
        }

     }

     if [type] == "users"{

        # warning : if TZ is not CEST, the filename must contain the TZ info. But not implemented yet
        grok { match => ["path", "(?<ADDED_DATE>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}_%{HOUR}-%{MINUTE}-%{SECOND})"]    } # get info form filename
        mutate { replace => ["ADDED_DATE","%{ADDED_DATE} +0200"] } # Add CEST timezone for declared users

        csv { # define user csv file, headers, with additionnal columns from logstash: "ADDED_DATE","COUNT","CLIENT"
            separator => ";"
            columns => ["USER","ADDED_DATE","COUNT","CLIENT"]
            remove_field => ["message"]
        }

        mutate { replace => ["CLIENT","${STACK_CLIENT}"] }
        mutate { replace => ["COUNT", 0] }
        #mutate { replace => ["type",'audit'] }

        date{         # Convert ADDED_DATE from string  into date type
            match => ["ADDED_DATE","YYYY-MM-dd_HH-mm-ss Z"]
            target => "ADDED_DATE"
        }

        mutate {
          add_field => { "fingerprint" => "%{USER}.%{ADDED_DATE}.%{CLIENT}" }
        }

     }

     if [type] == "shuttle" {

        mutate {
          gsub => [ "message", "^\b*", "" ] # remove space if exist on first char
        }

        fingerprint {
            source => "message"
            target => "fingerprint"
            method => "SHA1"
            key => "shuttle $ audit "  # keypassphrase to generate the fingerprint SHA1, dot not changeit
            base64encode => true
        }

        # Ex1 :   INFO 2019-03-27 02:00:53,848 [localhost-startStop-1] (MDMListener.java:67) com.nellarmonia.mdm.servlet.MDMListener - Starting Shuttle server

        grok { # test on https://grokconstructor.appspot.com/do/match#result
          match => [ "message", "%{LOGLEVEL:severity}%{SPACE}%{TIMESTAMP_ISO8601:timestamp}%{SPACE}\[(?<processname>.+)\](%{SPACE}\((?<sourcename>.+)\))?%{SPACE}%{JAVACLASS:class}%{SPACE}-%{SPACE}(?<message>[^\t]+)(?<stacktrace>(?m:.*))?" ]
          overwrite => ["message"] # remove the full message source
        }

        #grok {
        #  match => { "message" => "^%{JAVACLASS:exceptionclass}(: %{DATA:exceptiondata})?\n" }
        #}


        if [stacktrace] {
          grok {
            match => { "stacktrace" => ".*Caused by: %{JAVACLASS:causedby}(: (?<causedbydata>[^\t]+))?\n" }
            remove_tag => ["_grokparsefailure"]
          }
        }


        date {
          match => [ "timestamp" , "yyyy-MM-dd HH:mm:ss,SSS" ]
          timezone =>"${TZ}"
        }

        mutate {
        add_field => { "CLIENT" => "${STACK_CLIENT}" }
        }

     }

     # remove some internal tags, not usable for us , unless we have multiple host
     mutate { remove_field => [ "host", "@version" ] }
}
